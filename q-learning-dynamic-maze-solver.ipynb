{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44502a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "random.seed(2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ce939",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_list = [0, 1, 2, 3, 5, 6, 7, 8]\n",
    "#  [0, 1, 2\n",
    "#   3,    5\n",
    "#   6, 7, 8]\n",
    "\n",
    "time_list = [0, 1, 2]\n",
    "\n",
    "# the maze of size 201*201*2\n",
    "maze_cells = np.zeros((201, 201, 2), dtype=int)\n",
    "\n",
    "# load maze\n",
    "def load_maze():\n",
    "    if not os.path.exists(\"COMP6247Maze20212022.npy\"):\n",
    "        raise ValueError(\"Cannot find %s\" % file_path)\n",
    "\n",
    "    else:\n",
    "        global maze_cells\n",
    "        maze = np.load(\"COMP6247Maze20212022.npy\", allow_pickle=False, fix_imports=True)\n",
    "        maze_cells = np.zeros((maze.shape[0], maze.shape[1], 2), dtype=int)\n",
    "        for i in range(maze.shape[0]):\n",
    "            for j in range(maze.shape[1]):\n",
    "                maze_cells[i][j][0] = maze[i][j]  \n",
    "                # load the maze, with 1 denoting an empty location and 0 denoting a wall\n",
    "                maze_cells[i][j][1] = 0  \n",
    "                # initialized to 0 denoting no fire\n",
    "\n",
    "# get local 3*3 information centered at (x,y).\n",
    "def get_local_maze_information(x, y):\n",
    "    global maze_cells\n",
    "    random_location = random.choice(flag_list)\n",
    "    around = np.zeros((3, 3, 2), dtype=int)\n",
    "    for i in range(maze_cells.shape[0]):\n",
    "        for j in range(maze_cells.shape[1]):\n",
    "            if maze_cells[i][j][1] == 0:\n",
    "                pass\n",
    "            else:\n",
    "                maze_cells[i][j][1] = maze_cells[i][j][1] - 1  # decrement the fire time\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if x - 1 + i < 0 or x - 1 + i >= maze_cells.shape[0] or y - 1 + j < 0 or y - 1 + j >= maze_cells.shape[1]:\n",
    "                around[i][j][0] = 0  # this cell is outside the maze, and we set it to a wall\n",
    "                around[i][j][1] = 0\n",
    "                continue\n",
    "            around[i][j][0] = maze_cells[x - 1 + i][y - 1 + j][0]\n",
    "            around[i][j][1] = maze_cells[x - 1 + i][y - 1 + j][1]\n",
    "            if i == random_location // 3 and j == random_location % 3:\n",
    "                if around[i][j][0] == 0: # this cell is a wall\n",
    "                    continue\n",
    "                ran_time = random.choice(time_list)\n",
    "                around[i][j][1] = ran_time + around[i][j][1]\n",
    "                maze_cells[x - 1 + i][y - 1 + j][1] = around[i][j][1]\n",
    "    return around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deced1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the number of walls surrounding the agent\n",
    "\n",
    "def check_surroundings(state, position):\n",
    "    no_walls = 0\n",
    "    \n",
    "    for i in range(4):\n",
    "        if state[state_directions[i]] == 0 or tuple((position[0],position[1]) + x_y_directions[1:][i]) in imperfect_positions:\n",
    "            no_walls += 1\n",
    "            \n",
    "    return no_walls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the position after taking an action\n",
    "\n",
    "def take_action(position, action):\n",
    "        return (position[0] + action[0], position[1] + action[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2bcd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns true if the action results in hitting a wall or fire\n",
    "\n",
    "def enter_wall_or_fire(state, action):\n",
    "    new_pos = state[1 + action[0]][1 + action[1]]\n",
    "    if new_pos[0] == 0 or new_pos[1] > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updates the q table using the Bellman optimality equation\n",
    "\n",
    "def update_q_table(position, new_position, reward, idx):\n",
    "    next_idx = torch.argmax(q_table[new_position[0], new_position[1], :])\n",
    "\n",
    "    q_table[position[0], position[1], idx] = q_table[position[0], position[1], idx] + \\\n",
    "    lr * (reward + gamma * q_table[new_position[0], new_position[1], next_idx] \\\n",
    "             - q_table[position[0], position[1], idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fb16a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writes trace to output file\n",
    "\n",
    "def write_to_output(steps, position, action, total_reward, state):\n",
    "    with open('output.txt', 'a') as f:\n",
    "        \n",
    "        f.write(\"Step: \" + str(steps) +\n",
    "                \"  x: \" + str(position[0].item()) + \"  y: \" + str(position[1].item()) + \n",
    "                \"  Action taken: \" + str(tuple(action)) + \"  Total Reward: \" + str(total_reward) + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"State: \" + \"\\n\")\n",
    "        \n",
    "        f.write(str(state[0, 0]) + \" \" + str(state[0, 1]) + \" \" + str(state[0, 2]) + \"\\n\")\n",
    "        f.write(str(state[1, 0]) + \" \" + str(state[1, 1]) + \" \" + str(state[1, 2]) + \"\\n\")\n",
    "        f.write(str(state[2, 0]) + \" \" + str(state[2, 1]) + \" \" + str(state[2, 2]) + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a027f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent class which holds various useful methods\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, position):\n",
    "        self.position = position\n",
    "        self.state = None\n",
    "    \n",
    "    # returns the local maze information (observation) of agent\n",
    "    def retrieve_state(self):\n",
    "        self.state = get_local_maze_information(self.position[0], self.position[1])\n",
    "        return self.state\n",
    "    \n",
    "    # checks if agent is in position [1, 1]\n",
    "    def in_initial_position(self):\n",
    "        if self.position[0] == 1 and self.position[1] == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    # select action via epsilon decay policy\n",
    "    def choose_action(self, epsilon, train):\n",
    "        random_num = random.uniform(0, 1)\n",
    "        if random_num < epsilon and train:\n",
    "            action_idx = random.randint(0, no_actions - 1)\n",
    "        else:\n",
    "            action_idx = torch.argmax(q_table[self.position[0], self.position[1], :])\n",
    "\n",
    "        action = x_y_directions[action_idx]\n",
    "        \n",
    "        return action_idx, action\n",
    "    \n",
    "    # set position of agent when resetting from dead end\n",
    "    def set_position(self, position):\n",
    "        self.position = position\n",
    "        \n",
    "    # get position of agent\n",
    "    def get_position(self):\n",
    "        return self.position\n",
    "    \n",
    "    # check if agent is in position [199, 199]\n",
    "    def at_goal(self, new_position):\n",
    "        if new_position[0] == 199 and new_position[1] == 199:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training agent method\n",
    "\n",
    "def train_agent(train=True):\n",
    "    \n",
    "    # initialising the agent in position [1,1]\n",
    "    visited = [(1, 1)]\n",
    "    position = torch.tensor([1, 1])\n",
    "    agent = Agent(position)\n",
    "    \n",
    "    # keep track of reward\n",
    "    total_reward = 0\n",
    "    \n",
    "    # boolean to check when agent terminated at position [199, 199]\n",
    "    terminated = False\n",
    "\n",
    "    # stacks for intersections and intersection actions\n",
    "    intersections = []\n",
    "    intersection_actions = []\n",
    "\n",
    "    # number of steps\n",
    "    steps = 0\n",
    "\n",
    "    # take 5000 steps\n",
    "    while steps < 5000:\n",
    "        \n",
    "        # get position of agent and select action\n",
    "        position = agent.get_position()\n",
    "        state = agent.retrieve_state()\n",
    "        idx, action = agent.choose_action(epsilon, train)\n",
    "\n",
    "        # get no. of walls surrounding agent\n",
    "        no_walls = check_surroundings(state, position)\n",
    "\n",
    "        # no. of walls == 3 means dead end, reset agent position to last\n",
    "        # intersection and close off imperfect position leading to dead end\n",
    "        if no_walls == 3 and not agent.in_initial_position():\n",
    "            position = torch.tensor(intersections.pop())\n",
    "            intersection_action = intersection_actions.pop()\n",
    "            agent.set_position(position)\n",
    "            imperfect_pos = take_action(position, intersection_action)\n",
    "            imperfect_positions.append(imperfect_pos)\n",
    "            continue\n",
    "        elif no_walls == 1 or agent.in_initial_position(): # record all intersections and actions taken at them\n",
    "            intersections.append((position[0], position[1]))\n",
    "            intersection_actions.append(action)\n",
    "\n",
    "        # compute the next potential position\n",
    "        possible_position = position + action\n",
    "\n",
    "        # check if potential position is a bad state to be in\n",
    "        if (possible_position[0], possible_position[1]) in imperfect_positions or enter_wall_or_fire(state, action): # if possible position leads to dead end, fire or wall\n",
    "            new_position = position\n",
    "            reward = -20\n",
    "        elif (possible_position[0], possible_position[1]) in visited: # if possible position leads to already visited area\n",
    "            new_position = position\n",
    "            reward = -5\n",
    "        else:\n",
    "            new_position = possible_position # if possible position leads to new state\n",
    "            visited.append((possible_position[0], possible_position[1]))\n",
    "            reward = -1\n",
    "\n",
    "        # if agent reaches goal, terminate and reward 1000\n",
    "        if agent.at_goal(new_position):\n",
    "            reward = 1000\n",
    "            terminated = True\n",
    "\n",
    "        # if training, update the q_table, otherwise evaluate and write trace to output\n",
    "        if train:\n",
    "            update_q_table(position, new_position, reward, idx)\n",
    "        else:\n",
    "            write_to_output(steps, position, action, total_reward, state)\n",
    "\n",
    "        # set position of agent to new position\n",
    "        agent.set_position(new_position)\n",
    "\n",
    "        # accumulate the reward for this epoch and increment steps\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "        # break if terminated was set to true before\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    return position, total_reward, terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c693070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load maze\n",
    "load_maze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ef24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper variables and hyperparameters are set here\n",
    "no_rows = 199\n",
    "no_cols = 199\n",
    "\n",
    "no_actions = 5\n",
    "\n",
    "imperfect_positions = []\n",
    "q_table = torch.zeros((no_rows+1, no_cols+1, no_actions))\n",
    "\n",
    "# gamma discount factor\n",
    "gamma = 0.95\n",
    "\n",
    "# learning rate\n",
    "lr = 1\n",
    "\n",
    "# epsilon\n",
    "epsilon=0.5\n",
    "\n",
    "# epsilon decay\n",
    "epsilon_decay = 0.99\n",
    "\n",
    "state_directions = [(0, 1, 0), (1, 0, 0), (1, 2, 0), (2, 1, 0)]\n",
    "x_y_directions = np.array([(0, 0), (-1, 0), (0, -1), (0, 1), (1, 0)])\n",
    "\n",
    "epoch_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7745c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the training 300 times, recording positions, total epoch reward, etc.\n",
    "# also decay epsilon each epoch\n",
    "for epoch in range(300):\n",
    "    position, total_reward, terminated = train_agent(train=True)\n",
    "    epoch_rewards.append(total_reward)\n",
    "    print(\"Epoch: \", epoch, \" Epoch Reward: \", total_reward,\n",
    "          \" Last Position Reached: \", position, \" Goal Reached: \", terminated)\n",
    "    epsilon = epsilon * epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot total rewards across epochs\n",
    "plt.xlabel(\"Total Reward\")\n",
    "plt.ylabel(\"Epoch\")\n",
    "plt.ylim([-2100,0])\n",
    "plt.plot(epoch_rewards, 'c', alpha=1)\n",
    "plt.title('Training Agent')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8314260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our q-table\n",
    "torch.save(q_table, 'q_table.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf532f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our q-table\n",
    "q_table = torch.load('q_table.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b51957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our q-table\n",
    "f = open('output.txt', 'w')\n",
    "f.write('Reinforcement Learning for Dynamic Maze Solving - Trace Output File \\n\\n')\n",
    "f.write('Action Key: \\n' + '(0, 0)  = STAY \\n' + '(1, 0)  = LEFT \\n' + '(-1, 0) = RIGHT \\n' + '(0, -1) = UP \\n' + '(0, 1)  = DOWN \\n\\n')\n",
    "f.close()\n",
    "\n",
    "train_agent(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98881839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
